{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting warning to be off\n",
    "warnings.filterwarnings('ignore') # .select() <=> .iloc() suggestion\n",
    "\n",
    "# Importing data\n",
    "df = pd.read_table(\"Lucas_unique_directors_naturalperson_gender.csv\", header= None, sep= None, engine=\"python\")\n",
    "\n",
    "# Reshaping data frame\n",
    "data_prev = df.rename(columns=df.iloc[0])\n",
    "data_inter = data_prev.drop(labels=0,axis=\"index\")\n",
    "data = data_inter.drop(\"naturalperson\", axis=1)\n",
    "\n",
    "\n",
    "#data # Desired data frame shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Creating dummy variable 'natural person'\n",
    "\n",
    "__Logic:__ Using custom binary technique based on string attributes. \n",
    "   1. I am checking if column dmfullname has special addressing techniques (e.g. Sir, Dr., Madam ...);\n",
    "   2. I am checking if column dmfullname has any non-natural person attribute (e.g. numbers, punctuation);\n",
    "   3. I am checking if column dmfullname has any non-natural person addressing techniques for main regions mapped in the data(e.g. company entity registrations like GmbH or Ltd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    115982\n",
       "0      7530\n",
       "Name: FilterTitle, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First filter: Title identifier\n",
    "\n",
    "# Part 1: function special_person_address_identifier identifies which observations \n",
    "# have one of the titles options in dmfullname and creates intermediary columns named\n",
    "# after the title list option with 1 for positive, e.g. there is a title in dmfullnane,\n",
    "# or 0 if there is no.\n",
    "\n",
    "def special_person_address_identifier(address):\n",
    "    for i in address:\n",
    "        data[i] = np.where(data[\"dmfullname\"].str.contains(i), 1, 0)\n",
    "    \n",
    "titles = [\"Sir\", \"Madam\", \"Ms\", \"Mr\", \"Mrs\",\"Miss\", \"Dr\", \"Professor\"]\n",
    "special_person_address_identifier(titles)\n",
    "\n",
    "# Part 2: I have merge all intermediary title columns, e.g. Sir, into one column called 'TitleCheck'.\n",
    "# I have then used the same process as Part 1 to investigate which observations had a positive value of 1 and which not.\n",
    "# This would indicate then which have a positive 'FilterTitle' status. The ones that have are likely natural person while\n",
    "# the ones with a negative status, a.k.a 0 value, not.\n",
    "\n",
    "data[\"TitleCheck\"] = data[titles].apply(lambda row: \"+\" .join(row.values.astype(str)), axis=1)\n",
    "\n",
    "data[\"FilterTitle\"] = np.where(data[\"TitleCheck\"].str.contains(\"1\"), 1, 0)\n",
    "data = data.drop([\"Sir\", \"Madam\", \"Ms\", \"Mr\", \"Mrs\",\"Miss\", \"Dr\", \"Professor\",\"TitleCheck\"], axis=1)\n",
    "data[\"FilterTitle\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    123392\n",
       "1       120\n",
       "Name: FilterSymbols, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second filter: Non-natural person identifier (Numbers, Signals and other)\n",
    "\n",
    "def non_natural_identifier(symbols):\n",
    "    for i in symbols:\n",
    "        data[i] = np.where(data[\"dmfullname\"].str.contains(i), 1, 0)\n",
    "\n",
    "symbols_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "non_natural_identifier(symbols_list)\n",
    "\n",
    "data[\"SymbolsCheck\"] = data[symbols_list].apply(lambda row: \"+\" .join(row.values.astype(str)), axis=1)\n",
    "\n",
    "data[\"FilterSymbols\"] = np.where(data[\"SymbolsCheck\"].str.contains(\"1\"), 1, 0)\n",
    "data = data.drop([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"SymbolsCheck\"], axis=1)\n",
    "data[\"FilterSymbols\"].value_counts() # Result shows that 120 observations are certainly not natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    121992\n",
       "1      1520\n",
       "Name: FilterEntities, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third filter: Location / Region based company entity identifier\n",
    "data[\"dmcountry\"].value_counts()\n",
    "\n",
    "def company_legal_entity_identifier(entities):\n",
    "    for i in entities:\n",
    "        data[i] = np.where(data[\"dmfullname\"].str.contains(i), 1, 0)\n",
    "\n",
    "# Main regions considered: Italy, UK, Germany, France, Austria, USA, Netherlands, Sweden, \n",
    "#                          Hungary, Luxembourg, Norway, Finland, Ireland and Greece.\n",
    "regions_entity = [\"S.p.a.\", \"S.r.l.\", \"Ltd\", \"PLC\", \"B.V.\", \"Limited\", \"GmbH\", \n",
    "                       \"AG\", \"UG\", \"e.V.\", \"SAS\", \"SARL\", \"SA\", \"OG\", \"KG\", \"LLC\", \n",
    "                       \"U.A.\", \"C.V.\", \"AB\", \"HB\", \"KB\", \"Zrt.\", \"Kft.\", \"S.à r.l.\", \n",
    "                       \"S.A.\", \"SCSp\", \"LLCs\", \"Oy\", \"LTD\",\"L.T.D.\",\"P.C.\"]\n",
    "\n",
    "company_legal_entity_identifier(regions_entity)\n",
    "\n",
    "data[\"EntitiesCheck\"] = data[regions_entity].apply(lambda row: \"+\" .join(row.values.astype(str)), axis=1)\n",
    "\n",
    "data[\"FilterEntities\"] = np.where(data[\"EntitiesCheck\"].str.contains(\"1\"), 1, 0)\n",
    "data = data.drop([\"S.p.a.\", \"S.r.l.\", \"Ltd\", \"PLC\", \"B.V.\", \"Limited\", \"GmbH\", \n",
    "                       \"AG\", \"UG\", \"e.V.\", \"SAS\", \"SARL\", \"SA\", \"OG\", \"KG\", \"LLC\", \n",
    "                       \"U.A.\", \"C.V.\", \"AB\", \"HB\", \"KB\", \"Zrt.\", \"Kft.\", \"S.à r.l.\", \n",
    "                       \"S.A.\", \"SCSp\", \"LLCs\", \"Oy\", \"LTD\",\"L.T.D.\",\"P.C.\", \"EntitiesCheck\"], axis=1)\n",
    "\n",
    "data[\"FilterEntities\"].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intermediary Results:\n",
    "\n",
    "__Dummy Variables:__\n",
    "  1. __FilterTitle__ indicates, at current moment that, 115982 observations have one of the titles investigated, e.g. Sir, while 7530 doesnt. \n",
    "  2. __FilterSymbols__ indicates that 120 observations are certainly not natural persons because they have numeric values in dmfullname.\n",
    "  3. __FilterEntities__ indicates that 1520 observations have company legal entity names in dmfullname and are therefore certaily not natural persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83491"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for potential columns that have a good amount of observations filled.\n",
    "a = list(data[\"dmage\"].value_counts())\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = list(data[\"dmbirthdate\"].value_counts())\n",
    "sum(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119046"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = list(data[\"dmgender\"].value_counts())\n",
    "sum(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New filter ideas\n",
    "\n",
    "Columns dmage, dmbirthdate and dmgender are promising columns to reduce the number of observations I have to check manually once all these columns have more than 80000 filled. Based on this I will continue the filtering process of dummz variable natural_person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Age, Birthdate and Gender\n",
    "\n",
    "# Coming up with a way to represent empty cells under age , birthday and gender. In this case by filling empty cells with the value -1.\n",
    "data[\"FilterAge\"] = data[\"dmage\"].fillna(value=\"-1\")  \n",
    "data[\"FilterBirthDate\"] = data[\"dmbirthdate\"].fillna(value=\"-1\")\n",
    "data[\"FilterGender\"] = data[\"dmgender\"].fillna(value=\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M     98577\n",
       "F     20469\n",
       "-1     4466\n",
       "Name: FilterGender, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"FilterGender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary csv file with only gender (-1) observations. \n",
    "# EVERYTHING HAPPENING FROM HERE IN DATA: MANUAL, IS A TEST.\n",
    "manual = data[data[\"FilterGender\"] == \"-1\"]\n",
    "manual.to_csv(\"manual_gender_check.csv\") # Here, I would have to check 4446 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star rating logic\n",
    "\n",
    "Right now we have 6 filters.  Out of them, 2 are very secure, namely \"FilterSymbols\" & \"FilterEntities\", because if you have positive values there you are certainly a non-natural person.\n",
    "\n",
    "We remain with 4 filters, each has a dummy variable, consequently providing to us 2 variables - therefore, there are 8 variables in questions. There are possibly 8! permutations of these variables - very painful to condition.\n",
    "\n",
    "At the moment, I can either check 4466 observations manualy or try to scope down further.\n",
    "\n",
    "The scoping will follow a score mechanism based on only 3 filters: Age, BirthDate and Gender because Filtertitle can be the most misleading and laborious due to these facts: a lot observations to cover in the revision as well as to the possibility of existing companies such as \"Dr. Schwarz\" or who knows what else.\n",
    "\n",
    "__Score Structure__\n",
    "\n",
    "Score can range from 0 to 3 stars. 2 and 3 stars observations won't be revised manually. Observations gain stars based on the existence of a cell value for that attribute or not.\n",
    "\n",
    "Remember that if an observation had no value in its cell in the columns of FilterAge, FIlterBirthDate and FilterGender they were filled up with value -1. Therefore, the following conditions are being used:\n",
    "\n",
    "\n",
    "\n",
    "- Age != -1 , BirthDate != -1 , Gender != -1 -> __3 stars rating__\n",
    "- Age == -1 , BirthDate != -1 , Gender != -1 -> __2 stars rating__\n",
    "- Age != -1 , BirthDate == -1 , Gender != -1 -> __2 stars rating__\n",
    "- Age != -1 , BirthDate != -1 , Gender == -1 -> __2 stars rating__\n",
    "\n",
    "##### Cases that were revised manually\n",
    "- Age == -1 , BirthDate == -1 , Gender != -1 -> __1 stars rating__\n",
    "- Age == -1 , BirthDate != -1 , Gender == -1 -> __1 stars rating__\n",
    "- Age != -1 , BirthDate == -1 , Gender == -1 -> __1 stars rating__\n",
    "- Age == -1 , BirthDate == -1 , Gender != -1 -> __1 stars rating__\n",
    "- Age == -1 , BirthDate == -1 , Gender == -1 -> __0 stars rating__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of conditions\n",
    "conditions = [\n",
    "    (manual['FilterAge'] != \"-1\") & (manual['FilterBirthDate'] != \"-1\") & (manual['FilterGender'] != \"-1\"),\n",
    "    (manual['FilterAge'] == \"-1\") & (manual['FilterBirthDate'] != \"-1\") & (manual['FilterGender'] != \"-1\"),\n",
    "    (manual['FilterAge'] != \"-1\") & (manual['FilterBirthDate'] == \"-1\") & (manual['FilterGender'] != \"-1\"),\n",
    "    (manual['FilterAge'] != \"-1\") & (manual['FilterBirthDate'] != \"-1\") & (manual['FilterGender'] == \"-1\"),\n",
    "    (manual['FilterAge'] == \"-1\") & (manual['FilterBirthDate'] == \"-1\") & (manual['FilterGender'] != \"-1\"),\n",
    "    (manual['FilterAge'] == \"-1\") & (manual['FilterBirthDate'] != \"-1\") & (manual['FilterGender'] == \"-1\"),\n",
    "    (manual['FilterAge'] != \"-1\") & (manual['FilterBirthDate'] == \"-1\") & (manual['FilterGender'] == \"-1\"),\n",
    "    (manual['FilterAge'] == \"-1\") & (manual['FilterBirthDate'] == \"-1\") & (manual['FilterGender'] != \"-1\"),\n",
    "    (manual['FilterAge'] == \"-1\") & (manual['FilterBirthDate'] == \"-1\") & (manual['FilterGender'] == \"-1\"),\n",
    "]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['3', '2', '2', '2', '1', '1', '1', '1', '0']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "manual['score'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4153\n",
       "2     301\n",
       "1      12\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual[\"score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result Scoring:__ Based on the first score mechanism I can see that out of 4466 observations, 301 are scored with 2 stars and therefore secure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Score mechanism\n",
    "\n",
    "__Logic:__\n",
    "- Score != 2 & FilterEntity == 1 -> 'Non'\n",
    "- Score != 2 & FilterSymbols == 1 -> 'Non'\n",
    "- Score != 2 & FilterSymbols != 1 -> 'Non'\n",
    "- Score != 2 & FilterSymbols != 1 -> 'Non'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_cond = [(manual[\"FilterEntities\"] == 1) & (manual[\"score\"] != 2),\n",
    "            (manual[\"FilterSymbols\"] == 1) & (manual[\"score\"] != 2)\n",
    "            \n",
    "            \n",
    "]\n",
    "\n",
    "ext_values = [\n",
    "    \"non\", \"non\"\n",
    "]\n",
    "\n",
    "manual[\"ExtendedScore\"] = np.select(ext_cond, ext_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3683\n",
       "non     783\n",
       "Name: ExtendedScore, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual[\"ExtendedScore\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final extension to score mechanism\n",
    "final_conditions = [(manual[\"ExtendedScore\"] != \"non\") & (manual[\"FilterTitle\"] == 1),\n",
    "                    \n",
    "]\n",
    "\n",
    "final_values = [\n",
    "    \"natural\",\n",
    "]\n",
    "\n",
    "manual[\"ExtendedFinalScore\"] = np.select(final_conditions, final_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          4380\n",
       "natural      86\n",
       "Name: ExtendedFinalScore, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual[\"ExtendedFinalScore\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts on the preprocessing before I engage in manual work.\n",
    "\n",
    "### Remember that I am operating within the 4466 observations that are not certain.\n",
    "\n",
    "__Overview results:__\n",
    "\n",
    "- Encoding dmfullname based on natural person titles: 115982 observations have titles / 7530 observations have no titles.\n",
    "- Enconding dmfullname based on numeric symbols: 123392 observations have no numbers / 120 observations have numbers in dmfullname.\n",
    "- Encoding dmfullname based on entitity legal titles: 121992 observations have no entity titles / 1520 observations have entity titles in dmfullname.\n",
    "\n",
    "- Encoding dmAge, dmBirthDate & dmGender by checking if a value exists or not: \n",
    "    - dmGender -> Males: 98577 and Females: 20469 which together (a.k.a Natural) = 119046; -1 (a.k.a Non-natural and possible natural) = 4466\n",
    "    - dmAge -> Age given: 83491 , Age not given: Total Observations - 83491\n",
    "    - dmBirthDay -> BirthDay given: 82100, BirthDay not given: Total Observations - 82100\n",
    "\n",
    "__Upon Ecoding Strategies, summary of Scoring mechanism:__\n",
    "\n",
    "- 3 scores were created\n",
    "    - Column 'score' is a variable dependent on a given or not value encoding based on dmAge, dmGender and dmBirthDate; -> 301 observations less to manually check;\n",
    "    - Column 'ExtendedScore' is a variable dependent on a the scoring mechanism of column 'Score' and filters 'FilterSymbols' and 'FilterEntities'. -> 783 observations less to manually check;\n",
    "    - Column 'ExtendedFinalScore' is a variable dependent on scoring mechanism 'ExtendedScore' and column FilterTitle. -> 86 observations less to manually check.\n",
    "    \n",
    "### Final steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_conditions = [\n",
    "    (manual['ExtendedFinalScore'] == 'natural'),\n",
    "    (manual['ExtendedFinalScore'] != 'natural') & (manual['ExtendedScore'] == 'non'),\n",
    "    (manual['ExtendedFinalScore'] != 'natural') & (manual['ExtendedScore'] != 'non')  \n",
    "]\n",
    "\n",
    "score_value = [\"natural\",\n",
    "               \"non-natural\",\n",
    "               \"revision\"          \n",
    "]\n",
    "\n",
    "manual[\"NaturalPerson\"] = np.select(score_conditions, score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual[\"NaturalPerson\"].value_counts()\n",
    "manual.to_csv(\"revision.csv\") # Here, I would have to check 3597 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I need to revise 3597 instead of 4466 to define dummy variables NaturalPerson and GenderManual officialy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final steps for task:\n",
    "\n",
    "1. Embeded checked manual data with secure data.\n",
    "2. Finish 'NaturalPerson' encoding\n",
    "3. Create GenderManual dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging logic: \n",
    "\n",
    "I have merged both datas horizontally based on their index number which matched. This implies the following: data frame \"manual_checked\" had columns and values which were not part of data frame \"data\". The last operation merged the data and gave to the observations which were not consider in data frane \"manual_checked\" a NaN value. All the observations with NaN value are actual naturalPersons because they presented a gender attribute. These are the 119046 observations mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "natural        2290\n",
       "non-natural    2176\n",
       "Name: NaturalPerson, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_data = pd.read_table(\"revision_manual.csv\", sep=\",\", index_col=0)\n",
    "checked_data[\"NaturalPerson\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "natural        2290\n",
       "non-natural    2176\n",
       "Name: NaturalPerson, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.astype(str)\n",
    "checked_data = checked_data.astype(str)\n",
    "\n",
    "final = data.combine_first(checked_data) # CHECKING\n",
    "final[\"NaturalPerson\"].value_counts() # Merge numbers match numbers from \"checked_data\"\n",
    "#final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "natural        121336\n",
       "non-natural      2176\n",
       "Name: NaturalPerson, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[\"NaturalPerson\"] = final[\"NaturalPerson\"].fillna(\"natural\") # Filling, NaN observations with Natural because all of them had Gender.\n",
    "final[\"NaturalPerson\"].value_counts() # Done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: dummy variable \"NaturalPerson\" is done. \n",
    "\n",
    "__Result:__ 121336 observations are natural persons while 2176 aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: dummy variable: gender_manual\n",
    "\n",
    "__Logic:__ \n",
    "- 119046 observations have gender given. \n",
    "- 4466 are insecure observations, from which after manual checking them I can say, 2176 are non-natural persons.\n",
    "- 2290 should be checked / reviewed manually -> Here, I could check the data manually but there names I am not familiar with. For this reason, I have choosen to apply an algorithm to the column \"dmfirstname\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO: Separate the 2290 observation that are natural persons and apply algo in them to define their gender.\n",
    "genderize = checked_data[checked_data[\"NaturalPerson\"] == \"natural\"]\n",
    "#genderize # 2290 observations that are subjects of gender classification are ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderize[\"dmfirstname\"].value_counts().sum() # Out of those 2290, 2144 have a dmfirstname given. Which implies that 146 observations would still have to be checked manually.\n",
    "genderize[\"dmfirstname\"] = genderize[\"dmfirstname\"].fillna(value=\"revision\") # Marking all manual checks with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan        146\n",
       "Michael     16\n",
       "David       16\n",
       "John        13\n",
       "Janis       12\n",
       "          ... \n",
       "Brent        1\n",
       "Kate         1\n",
       "Karin        1\n",
       "Nika         1\n",
       "Greger       1\n",
       "Name: dmfirstname, Length: 1499, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genderize[\"dmfirstname\"].value_counts() # Checking progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_revision_manual = genderize[genderize[\"dmfirstname\"] == \"revision\"]\n",
    "gender_revision_manual.to_csv(\"gender_revision_manual.csv\") # Going manual\n",
    "\n",
    "# Merging rquirements\n",
    "gender_checked = pd.read_table(\"gender_revision_manual 2.csv\", sep=\",\", index_col=0)\n",
    "gender_checked = gender_checked.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge\n",
    "intermediary_obj = gender_checked.combine_first(genderize) # Updated and manually check no dmfirstname observations that are natural persons\n",
    "returning_intermediary_to_final = intermediary_obj.combine_first(final) # Adding back some mistakes regarding \"naturalpersons\". Previously 2176 non-natural, now 2195.\n",
    "\n",
    "df_apply_classifier = returning_intermediary_to_final[returning_intermediary_to_final[\"NaturalPerson\"] != \"non-natural\"]\n",
    "df_apply_classifier[\"dmfirstname\"].value_counts().sum() # Verification, that all my NaturalPerson have actually a dmfirstname to apply the gender classifier.\n",
    "\n",
    "df_apply_classifier_predict = df_apply_classifier[df_apply_classifier[\"dmgender\"] == \"nan\"]\n",
    "df_apply_classifier_names = df_apply_classifier_predict[\"dmfirstname\"]\n",
    "df_apply_classifier_names.value_counts().sum() # Checking the amount observations I have to predict and numbers matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits for the inspiration: https://github.com/Jcharis/Python-Machine-Learning/tree/master/Gender%20Classification%20With%20%20Machine%20Learning\n",
    "\n",
    "# Setup of classifier\n",
    "names_data = pd.read_csv('names_dataset.csv') # Total of: 181800 female names & 103275 male names\n",
    "names_data.sex.replace({'F':0,'M':1},inplace=True) # Female is 0, Male is 1\n",
    "\n",
    "Xfeatures =names_data['name']\n",
    "\n",
    "# Feature Extraction \n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(Xfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6398163206734908"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train Part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Labels\n",
    "y = names_data.sex\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n",
      "Female\n"
     ]
    }
   ],
   "source": [
    "### Test / Prediction Part: Female is 0, Male is 1\n",
    "\n",
    "def genderpredictor(a):\n",
    "    test_name = [a]\n",
    "    vector = cv.transform(test_name).toarray()\n",
    "    if clf.predict(vector) == 0:\n",
    "        print(\"Female\")\n",
    "    else:\n",
    "        print(\"Male\")\n",
    "        \n",
    "# Gender list that I want to predict in my model\n",
    "cake = [\"Mark\", \"Carla\"]\n",
    "\n",
    "bayes_classifier = {}\n",
    "\n",
    "for i in cake:\n",
    "    bayes_classifier[i] = genderpredictor(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190             Fonciere\n",
       "560               Enrico\n",
       "888            Gianbeppi\n",
       "990                 Marc\n",
       "1220              Thomas\n",
       "               ...      \n",
       "123505           Joannie\n",
       "123506             Julia\n",
       "123507    Triantaphyllos\n",
       "123508            Daniel\n",
       "123510         Anastasia\n",
       "Name: dmfirstname, Length: 2271, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_apply_classifier_names.value_counts() # There are 1557 different first names. \n",
    "df_apply_classifier_names.value_counts().sum() # Their ,sum() add up to 2271\\\n",
    "df_apply_classifier_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<63666x8194 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 381996 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By Analogy most female names ends in 'A' or 'E' or has the sound of 'A'\n",
    "def features(name):\n",
    "    name = name.lower()\n",
    "    return {\n",
    "        'first-letter': name[0], # First letter\n",
    "        'first2-letters': name[0:2], # First 2 letters\n",
    "        'first3-letters': name[0:3], # First 3 letters\n",
    "        'last-letter': name[-1],\n",
    "        'last2-letters': name[-2:],\n",
    "        'last3-letters': name[-3:],\n",
    "    }\n",
    "\n",
    "# Vectorize the features function\n",
    "features = np.vectorize(features)\n",
    "\n",
    "# Extract the features for the dataset\n",
    "df_X = features(names_data['name'])\n",
    "df_y = names_data['sex']\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    " \n",
    "corpus = features([\"Mike\", \"Julia\"])\n",
    "dv = DictVectorizer()\n",
    "dv.fit(corpus)\n",
    "transformed = dv.transform(corpus)\n",
    "dfX_train, dfX_test, dfy_train, dfy_test = train_test_split(df_X, df_y, test_size=0.33, random_state=42)\n",
    "dv = DictVectorizer()\n",
    "dv.fit_transform(dfX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building Using DecisionTree instead of NaiveBayes\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "dclf = DecisionTreeClassifier()\n",
    "my_xfeatures =dv.transform(dfX_train)\n",
    "dclf.fit(my_xfeatures, dfy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for n in namelist:\\n    classified[n] = genderpredictor1(n) '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting Gender of Name\n",
    "# Male is 1,female = 0\n",
    "\n",
    "# A function to do it\n",
    "def genderpredictor1(a):\n",
    "    test_name1 = [a]\n",
    "    transform_dv =dv.transform(features(test_name1))\n",
    "    vector = transform_dv.toarray()\n",
    "    if dclf.predict(vector) == 0:\n",
    "        print(\"Female\")\n",
    "    else:\n",
    "        print(\"Male\")\n",
    "        \n",
    "namelist = list(df_apply_classifier_names)\n",
    "classified = {}\n",
    "\n",
    "# Very dirty work. If there is time, recode it properly. What I did here: Ctrl + c to copy classification and adding manually to a .csv file.\n",
    "'''for n in namelist:\n",
    "    classified[n] = genderpredictor1(n) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888951716771903\n",
      "0.8665454893332057\n"
     ]
    }
   ],
   "source": [
    "## Accuracy of Models Decision Tree Classifier Works better than Naive Bayes\n",
    "\n",
    "# Accuracy on training set\n",
    "print(dclf.score(dv.transform(dfX_train), dfy_train))\n",
    "\n",
    "# Accuracy on test set\n",
    "print(dclf.score(dv.transform(dfX_test), dfy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan    2271\n",
       "Name: dmgender, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_apply_classifier_predict.to_csv(\"df_apply_classifier_predict.csv\")\n",
    "df_apply_classifier_predict[\"dmgender\"].value_counts() # Ctrl + C classification should be copied to this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    1448\n",
       "F     823\n",
       "Name: dmgender, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = pd.read_table(\"one.csv\", sep=\",\", index_col=0)\n",
    "predicted[\"dmgender\"].value_counts() # Result from the observations that were NaturalPersons, after manual check, and didt have firstname nor gender given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "returning_intermediary_to_final[\"dmgender\"] = returning_intermediary_to_final[\"dmgender\"].replace(\"\",\"NaN\")\n",
    "combiner = predicted.combine_first(returning_intermediary_to_final)\n",
    "#combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "natural        121317\n",
       "non-natural      2195\n",
       "Name: NaturalPerson, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returning_intermediary_to_final[\"NaturalPerson\"].value_counts()\n",
    "combiner[\"NaturalPerson\"].value_counts() # NaturalPersons checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M      100025\n",
       "F       21292\n",
       "nan      2195\n",
       "Name: dmgender, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returning_intermediary_to_final[\"dmgender\"].value_counts()\n",
    "combiner[\"dmgender\"].value_counts() # Gender checked checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODING OBSERVATIONS\n",
    "categories = [\"M\", \"F\"] # Males == 1; Females == 0;\n",
    "cleanup = [1, 0]\n",
    "\n",
    "persons = [\"natural\", \"non-natural\"] # Natural == 1; Non-natural == 0;\n",
    "\n",
    "combiner[\"GenderManual\"] = combiner[\"dmgender\"].replace(categories, cleanup)\n",
    "combiner[\"NaturalPerson\"] = combiner[\"NaturalPerson\"].replace(persons, cleanup)\n",
    "#combiner[\"NaturalPerson\"].value_counts()\n",
    "#combiner[\"GenderManual\"].value_counts()\n",
    "#combiner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANind & .CSV \n",
    "drops = [ 'ExtendedFinalScore',\n",
    " 'ExtendedScore',\n",
    " 'score',\n",
    " 'FilterAge',\n",
    " 'FilterBirthDate',\n",
    " 'FilterEntities',\n",
    " 'FilterGender',\n",
    " 'FilterSymbols',\n",
    " 'FilterTitle']\n",
    "\n",
    "dataFinal = combiner.drop(drops, axis=1)\n",
    "\n",
    "column_names = ['isin',\n",
    " 'v1',\n",
    " 'companynamelatinalphabet',\n",
    " 'bvdidnumber',\n",
    " 'name',\n",
    " 'cname',\n",
    " 'dmfullname',\n",
    " 'dmuciuniquecontactidentifie',\n",
    " 'dmjobtitleinenglish',\n",
    " 'dmjobtitle',\n",
    " 'dmappointmentdate',\n",
    " 'dmresignationdate',\n",
    " 'dmtitle',\n",
    " 'dmsalutation',\n",
    " 'dmfirstname',\n",
    " 'dmmiddlename',\n",
    " 'dmlastname',\n",
    " 'dmsuffix',\n",
    " 'dmgender',\n",
    " 'dmbirthdate',\n",
    " 'dmage',\n",
    " 'dmagebracket',\n",
    " 'dmcountryiesofnationality',\n",
    " 'dmaddress',\n",
    " 'dmcountry',\n",
    " 'dmemailaddress',\n",
    " 'dmbiography',\n",
    " 'dmtypeofrole',\n",
    " 'dmboardcommitteeordepartmen',\n",
    " 'dmlevelofresponsibility',\n",
    " 'dmalsoashareholder',\n",
    " 'dmconfirmationdates',\n",
    " 'dmdateslastreceivedfromip',\n",
    " 'dmnotvalidafterdate',\n",
    " 'dminformationsources',\n",
    " 'dminformationproviders',\n",
    " 'dmcollege',\n",
    " 'dmdegreecode',\n",
    " 'dmmajor',\n",
    " 'dmgraduationdate',\n",
    " 'dmcorp',\n",
    " 'gender',\n",
    " 'dmcorrespondingbvdidwhenapp',\n",
    " 'dmasanycategory',\n",
    " 'dmcurrentorprevious',\n",
    " 'dmbirthplace',\n",
    " 'dmhasasignatoryright',\n",
    " 'dmhasapowerofattorney',\n",
    " 'dmnoofcosinwhichacurrent',\n",
    " 'compensationsalaryeur', 'NaturalPerson', 'GenderManual']\n",
    "\n",
    "dataFinal = dataFinal.reindex(columns=column_names)\n",
    "#dataFinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFinal.to_csv(\"Lucas_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision - see video as well\n",
    "\n",
    "I realized I have worked only with the set of observations which were uncertain under the column \"dmgender\". The issue was that after working with this set I have not, doubled check the other set of column \"dmgender\" - indirectly assuming that the other set was reliable even thought we can't be sure of that.\n",
    "\n",
    "So what have to be done is: \n",
    "\n",
    "\n",
    "    Apply the same procedures to the other set of dmgender - the set of observations that gender given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    121317\n",
       "0      2195\n",
       "Name: NaturalPerson, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFinal[\"NaturalPerson\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    82087\n",
       "0    37821\n",
       "1     1409\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing set of natural persons to Score Mechanism - reduced version of 3starFilter to 2starFilter because Gender is given by all observations now.\n",
    "\n",
    "setNatural = dataFinal[dataFinal[\"NaturalPerson\"] == 1]\n",
    "\n",
    "setNatural = setNatural.astype(\"object\")\n",
    "\n",
    "setNatural[\"FilterAge\"] = setNatural[\"dmage\"] \n",
    "setNatural[\"FilterBirthDate\"] = setNatural[\"dmbirthdate\"]\n",
    "\n",
    "encode_nan = {\n",
    "    \"FilterAge\": {\"nan\": -1},\n",
    "    \"FilterBirthDate\": {\"nan\": -1}\n",
    "}\n",
    "\n",
    "setNatural.replace(encode_nan, inplace=True)\n",
    "\n",
    "#setNatural[\"FilterBirthDate\"].value_counts()\n",
    "#setNatural[\"FilterDate\"].value_counts()\n",
    "\n",
    "# List of conditions\n",
    "conditions_set_natural = [\n",
    "    (setNatural['FilterAge'] != -1) & (setNatural['FilterBirthDate'] != -1),\n",
    "    (setNatural['FilterAge'] == -1) & (setNatural['FilterBirthDate'] != -1),\n",
    "    (setNatural['FilterAge'] != -1) & (setNatural['FilterBirthDate'] == -1),\n",
    "    (setNatural['FilterAge'] == -1) & (setNatural['FilterBirthDate'] == -1),\n",
    "]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values_set_natural = ['2', '1', '1', '0']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "setNatural['Score'] = np.select(conditions_set_natural, values_set_natural)\n",
    "#setNatural = setNatural.drop(\"FilterGender\", axis=1)\n",
    "\n",
    "setNatural['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In between Interpretation \n",
    "\n",
    "So out of the \"natural persons\" set, there are 82087 with a 2 star rating (__adjust rating__, if I had consider Gender I would have received 3 star ratings in all of these) and there are 1409 observations with a 1 star rating (__adjust rating__, if I had consider Gender I would have received 2 star ratings in all of these). So there are 37821 observations that I could verify manually or scope down. \n",
    "\n",
    "- 2 star and 1 star observations don't need revision, because they are more likely to be correct. \n",
    "\n",
    "I will try to scope them down using 2 new filtes: FilterTitle and FilterEntities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "setNaturalrevision = setNatural[setNatural['Score'] == \"0\"]\n",
    "\n",
    "def special_person_address_identifier(address):\n",
    "    for i in address:\n",
    "        setNaturalrevision[i] = np.where(setNaturalrevision[\"dmfullname\"].str.contains(i), 1, 0)\n",
    "    \n",
    "titles = [\"Sir\", \"Madam\", \"Ms\", \"Mr\", \"Mrs\",\"Miss\", \"Dr\", \"Professor\"]\n",
    "special_person_address_identifier(titles)\n",
    "\n",
    "\n",
    "setNaturalrevision[\"TitleCheck\"] = setNaturalrevision[titles].apply(lambda row: \"+\" .join(row.values.astype(str)), axis=1)\n",
    "\n",
    "setNaturalrevision[\"FilterTitle\"] = np.where(setNaturalrevision[\"TitleCheck\"].str.contains(\"1\"), 1, 0)\n",
    "setNaturalrevision = setNaturalrevision.drop([\"Sir\", \"Madam\", \"Ms\", \"Mr\", \"Mrs\",\"Miss\", \"Dr\", \"Professor\",\"TitleCheck\"], axis=1)\n",
    "\n",
    "def company_legal_entity_identifier(entities):\n",
    "    for i in entities:\n",
    "        setNaturalrevision[i] = np.where(setNaturalrevision[\"dmfullname\"].str.contains(i), 1, 0)\n",
    "\n",
    "# Main regions considered: Italy, UK, Germany, France, Austria, USA, Netherlands, Sweden, \n",
    "#                          Hungary, Luxembourg, Norway, Finland, Ireland and Greece.\n",
    "regions_entity = [\"S.p.a.\", \"S.r.l.\", \"Ltd\", \"PLC\", \"B.V.\", \"Limited\", \"GmbH\", \n",
    "                       \"AG\", \"UG\", \"e.V.\", \"SAS\", \"SARL\", \"SA\", \"OG\", \"KG\", \"LLC\", \n",
    "                       \"U.A.\", \"C.V.\", \"AB\", \"HB\", \"KB\", \"Zrt.\", \"Kft.\", \"S.à r.l.\", \n",
    "                       \"S.A.\", \"SCSp\", \"LLCs\", \"Oy\", \"LTD\",\"L.T.D.\",\"P.C.\"]\n",
    "\n",
    "company_legal_entity_identifier(regions_entity)\n",
    "\n",
    "setNaturalrevision[\"EntitiesCheck\"] = setNaturalrevision[regions_entity].apply(lambda row: \"+\" .join(row.values.astype(str)), axis=1)\n",
    "\n",
    "setNaturalrevision[\"FilterEntities\"] = np.where(setNaturalrevision[\"EntitiesCheck\"].str.contains(\"1\"), 1, 0)\n",
    "setNaturalrevision = setNaturalrevision.drop([\"S.p.a.\", \"S.r.l.\", \"Ltd\", \"PLC\", \"B.V.\", \"Limited\", \"GmbH\", \n",
    "                       \"AG\", \"UG\", \"e.V.\", \"SAS\", \"SARL\", \"SA\", \"OG\", \"KG\", \"LLC\", \n",
    "                       \"U.A.\", \"C.V.\", \"AB\", \"HB\", \"KB\", \"Zrt.\", \"Kft.\", \"S.à r.l.\", \n",
    "                       \"S.A.\", \"SCSp\", \"LLCs\", \"Oy\", \"LTD\",\"L.T.D.\",\"P.C.\", \"EntitiesCheck\"], axis=1)\n",
    "\n",
    "#setNaturalrevision.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      34987\n",
       "0       2551\n",
       "non      283\n",
       "Name: ScoreBasedString, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of conditions\n",
    "conditions_set_natural_str = [\n",
    "    (setNaturalrevision['FilterTitle'] == 1) & (setNaturalrevision['FilterEntities'] != 1),\n",
    "    (setNaturalrevision['FilterTitle'] == 1) & (setNaturalrevision['FilterEntities'] == 1),\n",
    "    (setNaturalrevision['FilterTitle'] != 1) & (setNaturalrevision['FilterEntities'] == 1),\n",
    "    (setNaturalrevision['FilterTitle'] != 1) & (setNaturalrevision['FilterEntities'] != 1)\n",
    "    \n",
    "]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values_set_natural_str = ['1', 'non', 'non','0']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "setNaturalrevision['ScoreBasedString'] = np.select(conditions_set_natural_str, values_set_natural_str)\n",
    "setNaturalrevision['ScoreBasedString'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In between Interpretation\n",
    "\n",
    "\n",
    "So out of the 37821 observations which needed revision, I managed to scope them down to 2551 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_setNatural = setNaturalrevision[setNaturalrevision['ScoreBasedString'] == \"0\"]\n",
    "manual_setNatural.to_csv(\"manual_setNatural.csv\") # Going manual check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data after manual checked.\n",
    "manual_setNatural_checked = pd.read_table(\"manual_setNatural_checked.csv\", sep=\",\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2549\n",
       "0       2\n",
       "Name: NaturalPerson, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locating observations after manual check, to do a local change because of the small number of \"mistakes\"/ incorrect enconding.\n",
    "manual_setNatural_checked[\"NaturalPerson\"].value_counts() # After checking manually I found 2 observation out of the 2551 that are companies. I have changed them directly in the last dataframe.\n",
    "#manual_setNatural_checked[manual_setNatural_checked[\"NaturalPerson\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if my changes worked on \n",
    "\n",
    "checksdf = pd.read_table(\"Lucas_final_checked.csv\", sep=\",\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    100024\n",
       "0.0     21291\n",
       "Name: GenderManual, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checksdf[\"GenderManual\"].value_counts() # Checking if results matched, and they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
